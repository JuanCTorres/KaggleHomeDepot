{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "from ml_utils import Utils\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from pylab import figure, axes, title, savefig\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MIN_VAL = 1.0\n",
    "MAX_VAL = 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note about files being read:** These inputs are produced by running the `str_stem` function. They are essentially the files provided in the competition, but the strings have been pre-processed to eliminate irrelevant characters and other interferences, as well as to stem the strings, i.e., minimize the differences from inflexions found in the language (e.g., \"cat\" and \"cats\" are stemmed to the same word to maximize matches once the numerical features are calculated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../input/_final_train.csv\", encoding=\"ISO-8859-1\")\n",
    "df_test = pd.read_csv('../input/_final_test.csv', encoding=\"ISO-8859-1\")\n",
    "df_attr = pd.read_csv('../input/attributes.csv')\n",
    "df_prod_desc = pd.read_csv('../input/_final_desc.csv', index_col=\"product_uid\")\n",
    "ytrain = df_train[\"relevance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Unnamed: 0                                product_description\n",
      "product_uid                                                               \n",
      "100001                0  not on do angl make joint stronger they also p...\n",
      "100002                1  behr premium textur deckov is an innov solid c...\n",
      "100003                2  classic architectur meet contemporari design i...\n",
      "100004                3  the grape solar 265watt. polycrystallin pv sol...\n",
      "100005                4  updat your bathroom with the delta vero singl ...\n"
     ]
    }
   ],
   "source": [
    "# Make sure it worked\n",
    "print df_prod_desc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_scores(clf, verbose=False):\n",
    "    \"\"\"\n",
    "    Prints the best scores and best parameters found in cross-validation.\n",
    "    Takes:\n",
    "    - clf, a classifier that has been cross-validated by grid search.\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    print \"Best score and parameters:\"\n",
    "    pp.pprint(clf.best_score_)\n",
    "    pp.pprint(clf.best_params_)\n",
    "    \n",
    "    if verbose:\n",
    "        print \"~~~Verbose output~~~\"\n",
    "        print \"All scores\"\n",
    "        pp.pprint(clf.grid_scores_)\n",
    "    \n",
    "def fmean_squared_error(ground_truth, predictions):\n",
    "    \"\"\"\n",
    "    Returns the difference between the actual value and the predictions\n",
    "    output by a classifier\n",
    "    Takes:\n",
    "    - ground_truth, the actual value Y of the inputs X being predicted.\n",
    "    - predictions, the predicted value Yhat of the inputs X.\n",
    "    Returns:\n",
    "    - \n",
    "    \"\"\"\n",
    "    fmean_squared_error_ = mean_squared_error(ground_truth, predictions)**0.5\n",
    "    return fmean_squared_error_\n",
    "\n",
    "def is_prediction_in_range(yhat, min_val, max_val, verbose=False):\n",
    "    \"\"\"Returns whether the values of a prediction are in the expected range\"\"\"\n",
    "    return min(yhat) >= min_val and max(yhat) <= max_val\n",
    "\n",
    "def export_data(yhat, is_test, predictor_name):\n",
    "        output_folder = \"../output/\"\n",
    "\n",
    "        if not is_test:\n",
    "            # Most common words in data with the worst errors\n",
    "            df_word_cloud = pd.DataFrame(0, index=np.arange(len(df_train)),\\\n",
    "                                    columns=[\"product_title\", \"search_term\",\\\n",
    "                                         \"Y\", \"Yhat\", \"diff\"])\n",
    "\n",
    "            # Exports Y, Yhat and difference between them.\n",
    "            # Can be helpful for tuning or seeing patterns\n",
    "            df_num = pd.DataFrame(0, index=np.arange(len(df_train)),\\\n",
    "                                    columns=[\"Y\", \"Yhat\", \"diff\"])\n",
    "\n",
    "            # Word cloud\n",
    "            df_word_cloud[\"product_title\"] = df_train[\"product_title\"]\n",
    "            df_word_cloud[\"search_term\"] = df_train[\"search_term\"]\n",
    "            df_word_cloud[\"Y\"] = df_train[\"relevance\"]\n",
    "            df_word_cloud[\"Yhat\"] = yhat\n",
    "            df_word_cloud[\"diff\"] = (df_word_cloud[\"Y\"] - df_word_cloud[\"Yhat\"]) ** 2\n",
    "\n",
    "            # Diff\n",
    "            df_num[\"Y\"] = df_train[\"relevance\"]\n",
    "            df_num[\"Yhat\"] = yhat\n",
    "            df_num[\"diff\"] = df_word_cloud[\"diff\"]\n",
    "\n",
    "            output_file_words = output_folder + predictor_name + \"_word_cloud.csv\"\n",
    "            output_file_num = output_folder + predictor_name + \"_num.csv\"\n",
    "\n",
    "            df_word_cloud.to_csv(output_file_words, encoding=\"utf-8\")\n",
    "            df_num.to_csv(output_file_num, encoding=\"utf-8\")\n",
    "\n",
    "        else:\n",
    "            # Exports Yhat(testing data). This is the submission file\n",
    "            # df_result = pd.DataFrame(0, index=np.arange(len(df_test)),\n",
    "              #                      columns=[\"id\", \"relevance\"])\n",
    "\n",
    "            df_result = pd.DataFrame(yhat, columns=[\"relevance\"],\\\n",
    "                                    index=df_test[\"id\"])\n",
    "\n",
    "            output_file_yhat = output_folder + predictor_name + \"_yhat.csv\"\n",
    "\n",
    "            df_result.to_csv(output_file_yhat, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "RMSE = make_scorer(fmean_squared_error, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create numerical features\n",
    "### Simple feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_num_feat = pd.DataFrame(0, index=np.arange(len(df_train)), columns=[\"search_in_title\",\\\n",
    "                                     \"search_in_desc\"])\n",
    "test_num_feat = pd.DataFrame(0, index=np.arange(len(df_test)), columns=[\"search_in_title\",\\\n",
    "                                     \"search_in_desc\"])\n",
    "\n",
    "# Count the training data occurrences\n",
    "for i in range(0, df_train.shape[0]):\n",
    "    search = df_train[\"search_term\"][i]  # Search term used by user\n",
    "    title = df_train[\"product_title\"][i] # Product Title\n",
    "    product_id = df_train.iloc[[i]][\"product_uid\"][i]  # Get product_uid of current index in df_train\n",
    "    description = df_prod_desc.loc[product_id][\"product_description\"] # Description of product id\n",
    "    \n",
    "    for word in search.split():\n",
    "\n",
    "        if word in title:\n",
    "            train_num_feat[\"search_in_title\"][i] += 1\n",
    "            \n",
    "        if word in description:\n",
    "            train_num_feat[\"search_in_desc\"][i] += 1\n",
    "            \n",
    "# Count the testing data occurrences\n",
    "for i in range(0, df_test.shape[0]):\n",
    "    search = df_test[\"search_term\"][i]  # Search term used by user\n",
    "    title = df_test[\"product_title\"][i] # Product Title\n",
    "    product_id = df_test.iloc[[i]][\"product_uid\"][i]  # Get product_uid of current index in df_train\n",
    "    description = df_prod_desc.loc[product_id][\"product_description\"] # Description of product id\n",
    "\n",
    "    for word in search.split():\n",
    "        \n",
    "        if word in title:\n",
    "            test_num_feat[\"search_in_title\"][i] += 1\n",
    "        \n",
    "        if word in description:\n",
    "            test_num_feat[\"search_in_desc\"][i] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   search_in_title  search_in_desc\n",
      "0                1               1\n",
      "1                1               1\n",
      "2                1               1\n",
      "3                1               1\n",
      "4                3               2\n",
      "5                1               2\n",
      "6                2               2\n",
      "7                1               1\n",
      "8                2               2\n",
      "9                2               2\n"
     ]
    }
   ],
   "source": [
    "print train_num_feat.head(10)\n",
    "#train_num_feat.to_csv(\"../input/simple_train_num_feat.csv\")\n",
    "#test_num_feat.to_csv(\"../input/simple_test_num_feat.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or import them \n",
    "#### Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>search_in_title</th>\n",
       "      <th>search_in_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   search_in_title  search_in_desc\n",
       "0                1               1\n",
       "1                1               1\n",
       "2                1               2\n",
       "3                1               1\n",
       "4                3               3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_num_feat = pd.read_csv(\"../input/simple_train_num_feat.csv\", usecols=[1,2])\n",
    "test_num_feat = pd.read_csv(\"../input/simple_test_num_feat.csv\", usecols=[1,2])\n",
    "\n",
    "train_num_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Or more complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_len</th>\n",
       "      <th>title_len</th>\n",
       "      <th>desc_len</th>\n",
       "      <th>brand_len</th>\n",
       "      <th>query_in_title</th>\n",
       "      <th>query_in_desc</th>\n",
       "      <th>query_last_word_in_title</th>\n",
       "      <th>query_last_word_in_desc</th>\n",
       "      <th>word_in_title</th>\n",
       "      <th>word_in_desc</th>\n",
       "      <th>ratio_title</th>\n",
       "      <th>ratio_description</th>\n",
       "      <th>word_in_brand</th>\n",
       "      <th>ratio_brand</th>\n",
       "      <th>brand_feature</th>\n",
       "      <th>search_term_feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>135</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>135</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>169</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>109</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1010</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>109</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1010</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_len  title_len  desc_len  brand_len  query_in_title  query_in_desc  \\\n",
       "0          2          6       135          3               0              0   \n",
       "1          2          6       135          3               0              0   \n",
       "2          2         12       169          4               0              0   \n",
       "3          3         14       109          1               0              0   \n",
       "4          3         14       109          1               1              0   \n",
       "\n",
       "   query_last_word_in_title  query_last_word_in_desc  word_in_title  \\\n",
       "0                         0                        0              1   \n",
       "1                         0                        0              1   \n",
       "2                         0                        0              1   \n",
       "3                         0                        0              1   \n",
       "4                         1                        1              3   \n",
       "\n",
       "   word_in_desc  ratio_title  ratio_description  word_in_brand  ratio_brand  \\\n",
       "0             1        0.500              0.500              0         0.00   \n",
       "1             1        0.500              0.500              0         0.00   \n",
       "2             1        0.500              0.500              1         0.25   \n",
       "3             1        0.333              0.333              0         0.00   \n",
       "4             3        1.000              1.000              0         0.00   \n",
       "\n",
       "   brand_feature  search_term_feature  \n",
       "0           1000                   12  \n",
       "1           1000                    9  \n",
       "2           1000                    9  \n",
       "3           1010                   16  \n",
       "4           1010                   18  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_num_feat = pd.read_csv(\"../input/more_features/lab_x_train.csv\")\n",
    "test_num_feat = pd.read_csv(\"../input/more_features/lab_x_test.csv\")\n",
    "\n",
    "train_num_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model and predict both testing and training values\n",
    "### Hand-tuning parameters\n",
    "At first, I played with various parameters to the predictor and used it to predict training data. If the decision tree is not stopped early enough, the model will clearly overfit, resulting in a training MSE of ~0.001 with the complex feature set. I thought it would be an interesting experiment to try to limit the overfitting by matching the _training_ errors of the predictors that we had created with the best public _testing_ error so far (training MSE ~0.22). Interestingly enough, so far this hand-tuned version of the decision tree is the version that has produced the best public testing score out of all decision tree regressors we have tried, scoring better than the cross-validated model that should select better values for the parameters (see next section). This is, of course, not necessarily representative of the final performance of the models, since before the deadline the submissions are only scored on a portion of the testing data, but I thought it was interesting enough to mention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Yhat in range: True\n",
      "Testing Yhat in range: True\n",
      "Training MSE: 0.228578828314\n"
     ]
    }
   ],
   "source": [
    "clf = tree.DecisionTreeRegressor(min_samples_split=3, max_depth=6)\n",
    "# Train\n",
    "clf.fit(train_num_feat, df_train[\"relevance\"])\n",
    "\n",
    "# Predict training and testing data\n",
    "yhat_train_val = clf.predict(train_num_feat)\n",
    "yhat_test_val = clf.predict(test_num_feat)\n",
    "\n",
    "# Make sure results are within boundaries\n",
    "print \"Training Yhat in range: \" + str(is_prediction_in_range(yhat_train_val, MIN_VAL, MAX_VAL))\n",
    "print \"Testing Yhat in range: \" + str(is_prediction_in_range(yhat_test_val, MIN_VAL, MAX_VAL))\n",
    "# MSE\n",
    "err = (df_train[\"relevance\"] - yhat_train_val) ** 2\n",
    "mse_train = sum(err) / df_train.shape[0]\n",
    "print \"Training MSE: \" + str(mse_train)\n",
    "\n",
    "export_data(yhat_test_val, True, \"decision_tree_1_depth_lim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to cross validate, I will use `GridSearchCV`, a built-in scikit module that tries the different combinations of parameters that it is passed on a predictor, and stores the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the predictor\n",
    "tree_multi = tree.DecisionTreeRegressor()\n",
    "\n",
    "# Create parameter grid: Combinations of all of these parameters will be tried\n",
    "# with the predictor\n",
    "splitter = [\"best\", \"random\"]\n",
    "max_features = np.linspace(1, 16, 5).astype(int)\n",
    "max_depth = np.linspace(2, 10, 5)\n",
    "min_samples_split = np.linspace(1, 5, 5)\n",
    "min_samples_leaf = np.linspace(1, 5, 5)\n",
    "presort = [True, False]\n",
    "\n",
    "param_grid = dict(splitter=splitter, max_features=max_features,\\\n",
    "                  max_depth=max_depth,\\\n",
    "                  min_samples_split=min_samples_split,\\\n",
    "                  min_samples_leaf=min_samples_leaf,\\\n",
    "                  presort=presort\n",
    "                 )\n",
    "\n",
    "# Cross validate!\n",
    "reg_multi = GridSearchCV(tree_multi, param_grid=param_grid, scoring=RMSE)\n",
    "# Fit the final model on the training data\n",
    "reg_multi.fit(train_num_feat, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yhat_multi = reg_multi.predict(test_num_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters obtained from cross-validation follow. Run with `verbose=True` to see looong output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score and parameters:\n",
      "-0.48526348210881798\n",
      "{   'max_depth': 8.0,\n",
      "    'max_features': 12,\n",
      "    'min_samples_leaf': 4.0,\n",
      "    'min_samples_split': 2.0,\n",
      "    'presort': True,\n",
      "    'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "print_scores(reg_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export output and  some data for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    relevance\n",
      "id           \n",
      "1    2.051705\n",
      "4    2.070831\n",
      "5    2.070831\n",
      "6    2.806887\n",
      "7    2.235381\n"
     ]
    }
   ],
   "source": [
    "#export_data(yhat_train_val, \"decision_tree_depth_lim\")\n",
    "#my_new = Utils()\n",
    "#my_utils.export_data(yhat_multi, False, \"decision_tree_multi\")\n",
    "\n",
    "export_train_data(yhat_multi, True, \"decision_tree_multi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_utils = Utils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/grid_search.py:418: ChangedBehaviorWarning: The long-standing behavior to use the estimator's score function in GridSearchCV.score has changed. The scoring parameter is now used.\n",
      "  ChangedBehaviorWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 50  80 110]\n",
      "[[-0.42462729 -0.4012964  -0.353271   -0.38224442 -0.32380649]\n",
      " [-0.40840869 -0.47363578 -0.46290047 -0.43957141 -0.48610995]\n",
      " [-0.43593477 -0.44740311 -0.47697519 -0.46180781 -0.46838922]]\n",
      "[[-0.53362399 -0.58833978 -0.60228123 -0.59322528 -0.64438801]\n",
      " [-0.5335377  -0.51990067 -0.51768832 -0.53989008 -0.53184605]\n",
      " [-0.5180814  -0.52918906 -0.50461774 -0.52132608 -0.59077028]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.learning_curve import learning_curve\n",
    "#from sklearn.svm import SVC\n",
    "\n",
    "train_sizes, train_scores, valid_scores = learning_curve(reg_multi, train_num_feat, ytrain, train_sizes=[50, 80, 110], cv=5)            \n",
    "print train_sizes\n",
    "print train_scores\n",
    "print valid_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_sizes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0f7e203f0d32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mtrain_sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mtrain_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mvalid_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"~~~~\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_sizes' is not defined"
     ]
    }
   ],
   "source": [
    "print train_sizes\n",
    "print train_scores\n",
    "print valid_scores\n",
    "\n",
    "print \"~~~~\"\n",
    "print train_scores_mean\n",
    "print \"~~~~\"\n",
    "\n",
    "figure()\n",
    "plt.figure()\n",
    "plt.title(\"Cross-validation Error\")\n",
    "\n",
    "# In order to plot, need to get the average of all cross validation iterations\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "#train_scores_std = np.std(train_scores, axis=1)\n",
    "valid_scores_mean = np.mean(valid_scores, axis=1)\n",
    "#test_scores_std = np.std(valid_scores, axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "\n",
    "plt.plot(train_sizes, valid_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "\n",
    "savefig('foo.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " simpson strong tie 12 gaug angl\n",
      "   query_len  title_len  desc_len  brand_len  query_in_title  query_in_desc  \\\n",
      "0          2          6       135          3               0              0   \n",
      "\n",
      "   query_last_word_in_title  query_last_word_in_desc  word_in_title  \\\n",
      "0                         0                        0              0   \n",
      "\n",
      "   word_in_desc  ratio_title  ratio_description  word_in_brand  ratio_brand  \\\n",
      "0             0            0                  0              0            0   \n",
      "\n",
      "   brand_feature  search_term_feature  \n",
      "0           1000                   14  \n"
     ]
    }
   ],
   "source": [
    "print df_test.loc[0, \"product_title\"]\n",
    "print test_num_feat.iloc[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>product_uid</th>\n",
       "      <th>product_title</th>\n",
       "      <th>search_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>166692</th>\n",
       "      <td>166692</td>\n",
       "      <td>240760</td>\n",
       "      <td>224428</td>\n",
       "      <td>bosch 4in. bi metal hole saw</td>\n",
       "      <td>4in. hole saw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0      id  product_uid                 product_title  \\\n",
       "166692      166692  240760       224428  bosch 4in. bi metal hole saw   \n",
       "\n",
       "          search_term  \n",
       "166692  4in. hole saw  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.loc[[df_test.shape[0] - 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    Simpson Strong-Tie 12-Gauge Angle\n",
       "Name: product_title, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.loc[:, \"product_title\"].iloc[[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
